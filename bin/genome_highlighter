import argparse
import os
from subprocess import call
from pathlib import Path
import logging
from typing import TextIO
from datetime import datetime
import time
from pandas import DataFrame

from clustal_highlighter.modules.highlights import Highlights
from clustal_highlighter.modules.variant_handler import *
from clustal_highlighter.modules.file_handler import read_diverse_fimo_file, read_fasta_file
from clustal_highlighter.modules.file_handler import pwm_p_value_parse
from clustal_highlighter.modules.logger import *
LOGGER = logging.getLogger('genome_highlighter')

def generate_dependencies(args) -> Highlights:
    colors = ['blue', 'red']
    html_colors = ['Aqua', 'PaleVioletRed']
    working_directory = None
    reports_generated = 0
    reports_requested = 0

    if args.output_dir != None:
        os.makedirs(args.output_dir, exist_ok=True)
        absolute_path = os.path.expanduser(args.output_dir)
        working_directory = os.path.abspath(absolute_path)
    else:
        global_home = os.path.expanduser('~/highlights_output')
        args.output_dir = global_home
        if not os.path.exists(global_home):
            os.makedirs(global_home)
        working_directory = global_home
    
    per_motif_p_values = []
    if args.motif_files:
        motifs = generate_tuple_motifs(args.motif_files)
        for (motif_label, motif_file_path) in motifs:
            per_motif_p_values.extend(pwm_p_value_parse(motif_file_path))
        
    if args.csv:
        motif_counts_data = []
        motif_counts_columns = ['chromosome',
                                'sequence_start',
                                'sequence_stop',
                                'motif',
                                'count',
                                'nucleotides_covered',
                                'percent_covered',
                                'num_forward',
                                'num_backwards']
        
        fimo_sub_table = []
        fimo_sub_table_columns = ['chromosome',
                               'sequence_start',
                               'sequence_end',
                               'motif_start',
                               'motif_end',
                               'motif',
                               'matched_orientation',
                               'matched_p_value',
                               'matched_sequence']
        
        pwm_p_values_table_data = ['motif',
                                   'motif_definition_p_value']
        
        motif_coverage_data = []
        motif_coverage_columns = ['chromosome',
                                  'sequence_start',
                                  'sequence_stop',
                                  'motif_descriptor',
                                  'coverage']
        
        variant_data = []
        variant_columns = ['chromosome',
                           'sequence_start',
                           'sequence_stop',
                           'variant_position',
                           'ref_allele',
                           'alt_allele',
                           'ref_percent',
                           'alt_percent',
                           'accessibility']
        
        summary_data = []
        #% nucleotides N ONLY reference the fasta sequence passed, NOT the accessibility data
        summary_columns = ['report_file_path',
                           'chromosome',
                           'sequence_start',
                           'sequence_stop',
                           'num_variants',
                           'num_accessible_variants',
                           'num_nucleotides_accessible',
                           'pi_scores_mean',
                           'pi_scores_stdev', 
                           'genotype_mean_missing', 
                           'genotype_stdev_missing',
                           'num_nucleotides_N',
                           'num_nucleotides_highlighted']
        
        if args.motif_files:
            for motif in motifs:
                summary_columns.append(f'{motif[0]}_unique_motifs')
                summary_columns.append(f'{motif[0]}_total_motifs')
                summary_columns.append(f'num_{motif[0]}_nucleotide_coverage')
                    
    logging.basicConfig(filename=f'{working_directory}/{int(time.time())}.log', filemode='w', level=logging.DEBUG)

    info(LOGGER, f'Program Start Time: {datetime.now()}')
    
    print("Output and Working directory is at: " + working_directory)
    
    peak_locations = get_peak_locations(args.peaks)
    keys = list(peak_locations.keys())
    
    info(LOGGER, f'Successfully read in peaks from {args.peaks}. Peak keys as list: {keys}')
    for key in peak_locations:
        info(LOGGER, f'Chromosome {key} has {len(peak_locations[key])} peaks')

    print("Peaks loaded")

    print("Loading sequence file...")
    
    seq_dict = read_fasta_file(args.seq_file)
    all_seq_keys = list(seq_dict.keys())
        
    info(LOGGER, f'Successfully read in sequence file from: {args.seq_file}. Sequence keys as list: {all_seq_keys}')
    
    seq_keys = find_valid_keys(seq_dict, all_seq_keys,  keys, 'FASTA')
    
    print("Sequence file loaded")
    info(LOGGER, f'Removed excess sequences. Sequences saved: {seq_keys}')
    
    if args.accessibility:
        print("Loading in accessibility file")
        accessibility_dict = read_fasta_file(args.accessibility)
        all_accessibility_keys = list(accessibility_dict.keys())
        info(LOGGER, f'Successfully read in accessibility file from: {args.accessibility}. Accessibility keys as list: {all_accessibility_keys}')
        accessibility_keys = find_valid_keys(accessibility_dict, all_accessibility_keys, keys, 'Accessibility')
        print(f"Accessibility file loaded")
        info(LOGGER, f'Removed excess accessibility data. Accessibility data saved for these keys:{accessibility_keys}')
        if accessibility_keys == []:
            warning(LOGGER, f'Keys present in accessibility: {all_accessibility_keys}. VS key being loaded in from BED file: {keys}. Accessbility keys after validation were empty: {accessibility_keys} ')
            raise Exception('No accessibility sequence keys found that match provided keys found in BED file')    
    
    if seq_keys == []:
        warning(LOGGER, f'Keys present in seqeunces: {all_seq_keys}. VS key being loaded in from BED file: {keys}. Sequence keys after validation were empty: {seq_keys} ')
        raise Exception('No sequences found that match provided keys found in BED file')

    site_pi_path = None
    if args.site_pi:
        site_pi_path = f'{working_directory}/site_pi/diversity.sites.pi'
        if args.variant_data:
            print("Generating VCF Tools site pi data")
            vcf_file_extension = pathlib.Path(args.variant_data).suffix
            if vcf_file_extension == '.gz':
                    os.makedirs(os.path.dirname(f'{working_directory}/site_pi/'), exist_ok=True)
                    call(f"run_vcf_tools_pi {args.variant_data} 1 {working_directory}/site_pi/", shell=True)
            else:
                    os.makedirs(os.path.dirname(f'{working_directory}/site_pi/'), exist_ok=True)
                    call(f"run_vcf_tools_pi {args.variant_data} 0 {working_directory}/site_pi/", shell=True)
            info(LOGGER, f'Successfully generated site pi data at {working_directory}/stie_pi')
        else:
            warning(LOGGER, f'Site pi data was requested but --variant-data flag was not used. No variant or site pi data will be presented')
        
    vcf_df = pd.DataFrame()
    if args.variant_data:
        print("Loading VCF...")
        
        info(LOGGER, f'VCF added. Loading variant data from {args.variant_data}')
        
        vcf_df = read_vcf_into_dataframe(args.variant_data)
        
        info(LOGGER, f'From the VCF file provided, {vcf_df.shape[0]} rows (variants) and {vcf_df.shape[1]} columns were loaded')
        print("VCF Loaded")
    else:
        warning(LOGGER, 'No VCF given. Variant data will not be added.')

    chromosome = None
    if len(peak_locations.keys()) == 1:
        chromosome = list(peak_locations.keys())[0]

    for chromosome in peak_locations:
        print(f"Processing chromosome: {chromosome}")
        
        info(LOGGER, f'Chromosome {chromosome} being processed')
        
        if site_pi_path != None:
            diversity_dict = generate_pi_dict(site_pi_path)
        
        chromosome_seq = None
        file_name = None
        for file in seq_keys:
            if chromosome in file:
                chromosome_seq = seq_dict[file]
                file_name = file
      
        
        accessibility_seq = None
        if args.accessibility:
            for key in accessibility_dict:
                if chromosome in file:
                    accessibility_seq = seq_dict[file]
                    
        if accessibility_seq:
            if len(accessibility_seq) != len(chromosome_seq):
                warning(LOGGER, f'Accessibility sequence found for chromosome {chromosome} does not equal the length of the sequence found for that chromosome!')

        all_sequences = {}
        all_accessibility = {}
                
        for start, end in peak_locations[chromosome]:
            reports_requested += 1
            seq = chromosome_seq[start-1:end]
            if accessibility_seq:
                region_accessibility = accessibility_seq[start-1:end]
                all_accessibility[(start, end)] = region_accessibility

            single_seq_dict = {}
            single_seq_dict[f'{file_name}_{start}_{end}'] = seq

            all_sequences[(start, end)] = single_seq_dict
            
        info(LOGGER, f'For Chromosome {chromosome}, {reports_requested} sequences have been gathered')

        file_path = print_fasta(all_sequences, f'{working_directory}/{file_name}')

        fimo_highlights = []
        
        if args.motif_files:
            print("Generating fimo tsv")

            for motif_name, motif_path in motifs:
                motif_and_df, file_paths = generate_fimo_output(
                    file_path, motif_path, motif_name, working_directory)

                fimo_highlights.append(motif_and_df)
                info(LOGGER, f'Fimo successfully ran. File used: {motif_path}')
        
        print("Generating output...")
        for start, end in peak_locations[chromosome]:
            single_seq_dict = all_sequences[((start, end))]

            temp_highlight = Highlights(single_seq_dict, chromosome, start, end)
            
            temp_highlight.pwm_p_values = per_motif_p_values
            
            if accessibility_seq:
                region_accessibility = all_accessibility[(start,end)]
                # Must happen before adding variants! But I can't guarantee this...
                # because you can run without variants or without accessibility Ahhh!
                # pls don't break it whoever is reading this :(
                temp_highlight.add_accessibility(region_accessibility)
                
            if args.csv:
                temp_highlight.enable_csv(f'{file_name}/{file_name}-{start}-{end}.html')

            if args.variant_data and not vcf_df.empty:
                max_missing_frac = args.max_missing_frac
                min_allele_freq = args.min_allele_freq

                # Guard clauses to adjust data as necessary
                if max_missing_frac is not None:
                    verify_valid_decimals(max_missing_frac, "max-missing-frac")
                if min_allele_freq is not None:
                    verify_valid_decimals(min_allele_freq, "min-allele-freq")
            
                temp_highlight.add_variant_data(vcf_df, max_missing_frac, min_allele_freq)

                if site_pi_path != None:
                    temp_highlight.add_pi_data(diversity_dict)
                
            if fimo_highlights != []:
                for index, (motif_name, motif_df, working_directory) in enumerate(fimo_highlights):
                    seq_name = list(single_seq_dict.keys())[0]
                    motifs = motif_df.loc[motif_df['sequence_name'] == seq_name]
                    
                    motif_path = print_fimo_subset(motifs, working_directory)
                    temp_highlight.add_highlights(
                        motif_name, motif_path, colors[index], html_colors[index])
                    
            output_path = os.path.abspath(
                f'{args.output_dir}/{file_name}/{file_name}-{start}-{end}.html')

            if not os.path.exists(f'{args.output_dir}/{file_name}'):
                os.makedirs(f'{args.output_dir}/{file_name}')

            html_string_to_output(
                temp_highlight.generate_html_file(), output_path)
            reports_generated += 1
            
            if args.csv:
                if args.motif_files:
                    fimo_sub_table.extend(temp_highlight.fimo_sub_table)
                    motif_counts_data.extend(temp_highlight.motif_counts_csv)
                    motif_coverage_data.extend(temp_highlight.coverage_csv)
                    summary_data.extend(temp_highlight.summary_csv)
                if temp_highlight.varaints_csv: #varaints_csv defaults to none...
                    variant_data.extend(temp_highlight.varaints_csv)
                
        if args.csv:
            file_path = f'{args.output_dir}/csv/'
            if chromosome:
                file_path += f'{chromosome}/'
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            if motif_coverage_data != []:
                pd.DataFrame(motif_coverage_data, columns=motif_coverage_columns).to_csv(f'{file_path}/motif_coverage.csv', sep=',', index=False)
            if motif_counts_data != []:
                pd.DataFrame(motif_counts_data, columns=motif_counts_columns).to_csv(f'{file_path}/motif_counts.csv', sep=',', index=False)
            if variant_data != []:
                pd.DataFrame(variant_data, columns=variant_columns).to_csv(f'{file_path}/variant_data.csv', sep=',', index=False)
            if summary_data != []:
                pd.DataFrame(summary_data, columns=summary_columns).to_csv(f'{file_path}/summary.csv', sep=',', index=False)
            if fimo_sub_table != []:
                pd.DataFrame(fimo_sub_table, columns=fimo_sub_table_columns).to_csv(f'{file_path}/fimo_sub_table.csv', sep=',', index=False)
            if per_motif_p_values != {}:
                pd.DataFrame(per_motif_p_values, columns=pwm_p_values_table_data).to_csv(f'{file_path}/motif_definition_p_values.csv', sep=',', index=False)

            
    print(f'output at: {args.output_dir}')
    # Stats to verify everything is working
    #this needs to be in {} checks. I'll leave here for now ig...   
    if reports_generated == reports_requested:
        info(LOGGER, f'Sequenecs gathered. reports requested: {reports_requested} vs reports generated: {reports_generated}')
    else:
        warning(LOGGER, f'Sequenecs gathered. reports requested: {reports_requested} vs reports generated: {reports_generated}') 
    
    #Do all of my check validations here
    check_warnings()
    
def find_valid_keys(seq_dict, seq_keys, keys, description):
        valid_keys = []
        for seq_key in seq_keys:
            for key in keys:    
                if key in seq_key:
                    valid_keys.append(seq_key)
        if valid_keys == []:
            error_desc = f"Mismatch between provided keys. Keys given in bed file {keys} do not match keys given in {description} file {seq_keys}"
            warning(LOGGER, error_desc)
            raise Exception(error_desc)
        
        for seq_key in seq_keys:
            if seq_key not in valid_keys:
                del seq_dict[seq_key]
            else:
                seq_dict[seq_key] = seq_dict.pop(seq_key)
                        
        seq_keys = list(seq_dict.keys())
        return seq_keys
    

def add_log(log_file: TextIO, log: str):
    log_file.write(f'{log}\n', 'a')

def open_logfile(workspace, filename):
    log_file = open(f'{workspace}{filename}', 'w')
    return log_file

def print_fimo_subset(motif_df: DataFrame, working_directory ):
    file_path = f'{working_directory}/fimo_subset.tsv'
    motif_df.to_csv(file_path, index=False, sep="\t", header=True)
    return file_path

def print_fasta(all_sequences, output_path):
    file_path = f'{output_path}.fasta'
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    out = open(file_path, 'w')
    for key in all_sequences:
        start, end = key
        sequence_dict = all_sequences[key]
        for seq_name in sequence_dict:
            seq = sequence_dict[seq_name]
            out.write(f'>{seq_name}\n{seq}\n')

    out.close()
    return file_path

def generate_tuple_motifs(motifs):
    motif_pairs = []
    for i, j in zip(motifs[0::2], motifs[1::2]):
        j = os.path.abspath(j)
        motif_pairs.append((i, j))

    return motif_pairs


def generate_fimo_output(seq_path, motif_path, motif_name, output_path):
    seq_path = os.path.abspath(seq_path)
    basename_no_ending = Path(seq_path).stem
    file_paths = set()
    output_path = output_path + "/fimo"
    fimo_directory = f"{output_path}/{basename_no_ending}/{motif_name}/"
    os.makedirs(os.path.dirname(fimo_directory), exist_ok=True)
    call(f"run_fimo_on_file {seq_path} {motif_name} {motif_path} {output_path}", shell=True)

    fimo_file = fimo_directory + "/fimo.tsv"
    fimo_df = read_diverse_fimo_file(fimo_file)
    
    return (motif_name, fimo_df, fimo_directory), file_paths


def verify_valid_decimals(value, value_desc):
    if value < 0 or value > 1:
        raise argparse.ArgumentTypeError(
            f"{value} is an invalid value for {value_desc}")


def parseargs():
    """
    generates and returns a namespace for the argument parser
    """
    parser = argparse.ArgumentParser(description="Caller")

    parser.add_argument("--seq-file",
                        type=str,
                        required=True,
                        help="Path to fasta formatted sequence file ")

    parser.add_argument("--motif-files",
                        type=str,
                        required=False,
                        nargs='+',
                        help="A list of motif_name motif_file_path tuples for all the motifs that you want in the output")

    parser.add_argument("--variant-data",
                        required=False,
                        help="Path to VCF file containing variant data")

    parser.add_argument("--peaks",
                        required=True,
                        help="Path to BED file containing the locations of the different regions that you want highlights of (i.e. the start and end of an enhancer region")

    parser.add_argument("--output-dir",
                        type=str,
                        required=True,
                        help="Output file directory. This will be used as a working directory as well")

    parser.add_argument("--max-missing-frac",
                        type=float,
                        nargs='?',
                        const=None,
                        required=False,
                        help="0-1 float representing what percent of missing allele information makes a variant not display")

    parser.add_argument("--min-allele-freq",
                        type=float,
                        nargs='?',
                        const=None,
                        required=False,
                        help="0-1 float representing what minimum percent appearance an allele must have in order for a variant to display")

    parser.add_argument("--csv",
                        required=False,
                        action = 'store_true',
                        help="When present this flag allows for the generation of csv files which collect statistics shown in HTML files")
    
    parser.add_argument("--accessibility",
                        required=False,
                        help="A fast formatted file that contains per nucleotide accessibility data")
    
    parser.add_argument("--site-pi",
                        required=False,
                        action = 'store_true',
                        help="Calculates and displays vcftools site pi data in the results html files. Requires --variant-data run")
    
    return parser.parse_args()


def html_string_to_output(html_string, output_dir):
    # just writes the html string to out
    output = open(output_dir, 'w')
    output.write(html_string)
    output.close


if __name__ == "__main__":
    args = parseargs()
    if (args.motif_files != None) and (len(args.motif_files) % 2 != 0):
        print("\n--motif-files flag given an odd number of inputs, please use -h for more info")
        exit(0)
    else:
        generate_dependencies(args)
